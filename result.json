{"https://arxiv.org/abs/2402.10491": {"title_datetime_url": ["Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation", "2024.02.16", "https://guolanqing.github.io/Self-Cascade/"], "mechanism": "Nanyang&nbsp;Technological&nbsp;University:&nbsp;NTU||Tencent&nbsp;AI&nbsp;Lab:&nbsp;Tencent&nbsp;AI&nbsp;Lab||The&nbsp;Hong&nbsp;Kong&nbsp;University&nbsp;of&nbsp;Science&nbsp;and&nbsp;Technology:&nbsp;HKUST||Clemson&nbsp;University:&nbsp;Clemson", "summary": "本文提出了一种名为自级联扩散模型（Self-Cascade Diffusion Model）的新方法，旨在解决在生成不同分辨率图像时，扩散模型面临的构图挑战。该模型利用已训练好的低分辨率模型的丰富知识，通过调整或低成本的上采样调优策略，快速适应高分辨率图像和视频生成。模型通过集成多尺度上采样模块，能够在保持原始构图和生成能力的同时，高效适应更高分辨率。此外，提出了一种以枢轴引导的噪声重排策略，以加速推理过程并改善局部结构细节。与全微调相比，该方法在训练速度上提高了5倍，并且仅需要额外的0.002M调优参数。广泛的实验表明，该方法能够在仅进行10k步微调的情况下，快速适应高分辨率图像和视频合成，且几乎不需要额外的推理时间。\n\n自级联扩散模型的核心在于循环重用低分辨率扩散模型，通过枢轴引导的噪声重排策略，逐步适应更高分辨率的生成。该策略通过确定性缩放插值函数将低分辨率图像上采样，然后通过扩散过程生成高分辨率图像。为了进一步提高合成性能，特别是对于未见过的高分辨率真实图像的详细低级结构，模型引入了时间感知特征上采样器，这些上采样器可以灵活地插入到任何基于扩散的合成方法中，以实现更灵活的高分辨率图像或视频适应。这些上采样器简单轻量，由一个双线性上采样操作和两个残差块组成，总共只需要0.002M可训练参数。\n\n实验结果表明，自级联扩散模型在图像和视频合成中都取得了最先进的性能。在图像生成方面，模型在Laion-5B数据集上的4倍分辨率适应中，与全微调和LORA调优方法相比，展示了更快的训练速度和更好的性能。在视频生成方面，模型在Webvid-10M数据集上的4倍视频规模适应中，也显示出优于全微调和LORA调优方法的性能。这些结果证明了自级联扩散模型在高分辨率适应方面的有效性和效率。尽管如此，模型在处理极端案例时仍有局限性，例如在处理非常精细的物体或纹理重复时可能会遇到困难。未来的工作将探索适应效率与泛化能力之间的权衡。"}, "https://arxiv.org/abs/2402.10821": {"title_datetime_url": ["Training Class-Imbalanced Diffusion Model Via Overlap Optimization", "2024.02.16", "https://github.com/yanliang3612/DiffROP"], "mechanism": "Fudan&nbsp;University:Fudan||University&nbsp;of&nbsp;California&nbsp;Merced:UC&nbsp;Merced||CompVis&nbsp;Group:CompVis||LMU&nbsp;Munich:LMU||Google&nbsp;Research:Google", "summary": "这篇文章提出了一种名为DiffROP（Diffusion framework with Regularize OverlaP）的新方法，旨在解决在训练基于扩散模型的图像合成任务中，由于数据集不平衡导致的类别偏差问题。扩散模型在生成高质量图像方面取得了显著进展，但在处理真实世界数据集时，尤其是那些遵循长尾分布的数据集，对于尾部类别（rare classes）的图像生成质量通常较差。为了改善这一问题，DiffROP通过对比学习来最小化不同类别合成图像分布之间的重叠。这种方法通过引入概率对比学习（PCL）损失，惩罚不同类别的条件图像分布之间的KL散度，从而提高了尾部类别图像的生成质量，同时保持头部类别（head classes）的质量。\n\nDiffROP框架易于实现，并且可以应用于任何条件类扩散模型。它通过在训练过程中对比去噪后的图像来有效最小化PCL损失，而不需要像其他对比学习方法那样为正负实例设计专门的批量采样方案。此外，DiffROP还探索了多种PCL损失的变体，并通过可视化和详尽的消融研究来验证其有效性。实验结果表明，DiffROP显著提高了在不平衡数据集上训练的扩散模型的性能，尤其是在尾部类别上。\n\n文章还详细介绍了DiffROP的训练算法，包括如何通过时间依赖的τ参数和基于铰链的损失函数来优化PCL损失。在CIFAR10/CIFAR100等基准数据集上的实验结果显示，DiffROP在提高图像多样性和质量方面取得了显著成效。此外，DiffROP还展示了其在生成数据增强方面的潜力，这对于下游任务如图像分类具有重要意义。总的来说，DiffROP为处理长尾数据分布中的类别不平衡问题提供了一种有效且通用的解决方案。"}, "https://arxiv.org/abs/2402.10404": {"title_datetime_url": ["Explaining generative diffusion models via visual analysis for interpretable decision-making process", "2024.02.16", "https://doi.org/10.1016/j.eswa.2024.123231"], "mechanism": "Korea&nbsp;University:&nbsp;KU", "summary": "本研究旨在通过视觉分析解释生成性扩散模型（Generative Diffusion Models），以提高模型决策过程的可解释性。研究者提出了三个关键问题：模型在去噪过程中恢复了哪些区域（语义和细节层面），在每个去噪步骤中优先考虑了哪些具体概念，以及在时间步t中隐含了哪些视觉概念。为了回答这些问题，研究者开发了三种可视化工具：DF-RISE（Diffusion Randomized Input Sampling Explanation）、DF-CAM（Diffusion gradient-weighted Class Activation Mapping）和指数时间步采样。这些工具分别用于分析模型在去噪过程中的外部和内部工作机制，以及在不同时间阶段的视觉概念。\n\n通过实验，研究者发现扩散模型首先从包含语义信息的区域开始恢复图像，并逐渐向更精细的细节区域扩散。DF-CAM工具揭示了模型在每个去噪步骤中关注的具体视觉概念，而指数时间步采样则允许研究者在特定时间阶段集中分析模型的决策过程。这些发现不仅提供了对扩散过程的深入理解，而且为进一步研究可解释的扩散机制铺平了道路。\n\n研究还通过定量和定性评估验证了所提出工具的有效性。通过删除和插入游戏（Deletion and Insertion Games）以及相关性评分，研究者量化了视觉概念在每个步骤中的激活程度。这些评估结果表明，所提出的可视化方法能够有效地解释模型的决策过程，并为理解模型如何从噪声中生成高保真图像提供了新的视角。尽管如此，研究者指出，模型在处理多样化提示时的鲁棒性仍有待提高，且未来的研究应探索超越视觉概念层面的采样方法，以编辑图像生成中可能出现的不合理概念。"}, "https://arxiv.org/abs/2402.10855": {"title_datetime_url": ["Control Color: Multimodal Diffusion-based Interactive Image Colorization", "2024.02.16", "https://zhexinliang.github.io/Control_Color/;"], "mechanism": "S-Lab,&nbsp;Nanyang&nbsp;Technological&nbsp;University:&nbsp;S-Lab,&nbsp;NTU", "summary": "Control Color (CtrlColor) 是一种基于多模态扩散模型的交互式图像着色方法，由南洋理工大学的研究人员提出。这种方法利用预训练的稳定扩散（Stable Diffusion, SD）模型，实现了高度可控的图像着色。它支持无条件着色、基于文本提示、基于笔触和基于示例图像的着色，并且允许这些条件的任意组合。CtrlColor 通过使用笔触作为遮罩，实现了对图像局部区域的精确着色控制，并引入了一种基于自注意力和内容引导的可变形自编码器的新模块，以解决颜色溢出和颜色不准确的问题。实验表明，CtrlColor 在图像着色的质量、稳定性和视觉吸引力方面均优于现有技术。\n\nCtrlColor 的核心创新在于其能够通过笔触控制局部着色区域，同时通过自注意力引导和内容引导的可变形自编码器来处理颜色溢出和不准确的问题。这些设计使得用户可以轻松地对图像进行迭代编辑，精细调整着色过程中的具体细节。此外，CtrlColor 还支持将颜色化过程与文本提示、笔触和示例图像相结合，为用户提供了一种简单直观的方式来根据个人喜好对图像进行着色。\n\n在实验部分，CtrlColor 在多个公开的图像着色数据集上进行了评估，包括 COCO-Stuff 和 ImageNet。与现有的图像着色方法相比，CtrlColor 在颜色丰富度、颜色溢出问题处理以及与文本描述的一致性方面表现出色。用户研究结果进一步证实了 CtrlColor 在创造性艺术和老照片修复领域的应用潜力，用户对其生成的着色结果的满意度较高。尽管 CtrlColor 在处理小区域着色和复杂颜色示例时可能存在局限性，但其在图像着色领域的贡献和潜力是显而易见的。"}, "https://arxiv.org/abs/2402.10294": {"title_datetime_url": ["LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing", "2024.02.15", "https://doi.org/10.1145/3640543.3645143"], "mechanism": "University&nbsp;of&nbsp;Toronto:&nbsp;UofT||Reality&nbsp;Labs&nbsp;Research,&nbsp;Meta:&nbsp;Meta||University&nbsp;of&nbsp;California&nbsp;San&nbsp;Diego:&nbsp;UCSD", "summary": "LAVE是一个创新的视频编辑工具，它结合了大型语言模型（LLM）和语言增强功能，旨在降低视频编辑的门槛，尤其是对于初学者。该系统通过自动生成视频内容的文本描述（视觉叙述），利用LLM的叙事和推理能力来协助用户完成编辑任务。用户可以通过与LLM代理进行自然语言对话来获取帮助，或者直接通过用户界面（UI）进行操作。LAVE支持多种编辑功能，包括视频概述、创意头脑风暴、视频检索、故事板制作和剪辑修剪。这些功能通过LLM的提示工程技术实现，用户可以通过简单的语言命令来执行复杂的编辑操作。\n\nLAVE的用户研究显示，该系统能够有效地帮助用户完成视频编辑任务，并且用户对其功能表示赞赏。研究参与者，无论是视频编辑新手还是熟练者，都能在系统中产生满意的视频成果。用户对LLM代理的接受度较高，他们认为代理在保持用户主导权的同时，提供了有价值的协助。然而，研究也揭示了LLM在创意过程中可能引入的偏见问题，以及用户对LLM能力的理解和信任程度对使用体验的影响。\n\n尽管LAVE展示了LLM在视频编辑中的潜力，但仍存在一些局限性，如LLM的上下文窗口限制、生成的文本可能存在事实不准确的问题，以及用户对LLM的先验知识可能影响其使用体验。未来的工作可以在这些领域进行改进，例如开发多代理系统、提供更精细的编辑控制、整合视觉语言模型（VLMs）以及进行更广泛的用户评估。LAVE的研究为未来多媒体内容编辑工具的设计提供了宝贵的启示，特别是在如何利用LLM和代理技术来增强用户的创造力和编辑体验方面。"}, "https://arxiv.org/abs/2402.10636": {"title_datetime_url": ["PEGASUS: Personalized Generative 3D Avatars with Composable Attributes", "2024.02.16", ""], "mechanism": "Seoul&nbsp;National&nbsp;University:SNU", "summary": "PEGASUS（Personalized Generative 3D Avatars with Composable Attributes）是一种从单目视频源构建个性化生成3D面部化身的方法。该方法允许用户选择性地改变目标个体的面部属性（如发型或鼻子），同时保持其身份特征。PEGASUS通过构建一个合成视频集合来实现这一目标，这些视频集合通过从其他单目视频中借用不同个体的部分来合成。这种方法通过实验展示了在生成未见过的属性时具有高度真实性。此外，PEGASUS还引入了一种零样本（zero-shot）方法，通过利用先前构建的个性化生成模型，更高效地实现相同的生成建模。\n\nPEGASUS的核心是其个性化生成模型，它接受潜在代码作为输入，并结合FLAME参数来生成3D化身。模型通过多阶段的规范化空间和点变形来处理身份和外观变化，同时允许对特定面部组件进行控制。为了构建这个模型，研究者们提出了一种通过部分交换（part-swapping）合成数据集的方法，这涉及到从其他视频源中提取面部属性并将其与目标个体的视频合成。这种方法使得模型能够在没有额外训练的情况下，通过零样本转移（zero-shot transfer）技术，将面部属性从一个个体转移到另一个个体。\n\n实验结果表明，PEGASUS在保持目标个体身份的同时，能够生成具有高度自然性和真实感的3D化身。与基线方法相比，PEGASUS在生成多样化的面部属性方面表现出色，尤其是在零样本转移的应用中，能够无缝地将新属性融入目标个体的化身中。尽管如此，PEGASUS的个性化化身质量尚未达到照片级真实感，且在实现物理准确性方面存在局限性，特别是在替换面部特征如鼻子、嘴巴和眼睛时。未来的工作可能会集中在提高生成模型的质量和扩展模型以包含更多身份。"}, "https://arxiv.org/abs/2402.10259": {"title_datetime_url": ["GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting", "2024.02.15", "https://gaussianobject.github.io/"], "mechanism": "Shanghai&nbsp;Jiao&nbsp;Tong&nbsp;University:SJTU||Huawei:Huawei||University&nbsp;of&nbsp;Toronto:University&nbsp;of&nbsp;Toronto", "summary": "该研究介绍了一个名为GaussianObject的框架，它能够仅通过四张输入图像重建高质量的三维物体。GaussianObject利用高斯溅射（Gaussian Splatting）技术，结合视觉外壳和浮点消除技术，有效地解决了从稀疏视角重建三维物体时面临的两个主要挑战：构建多视图一致性和处理视角覆盖不足导致的物体信息遗漏或压缩。通过这些技术，GaussianObject能够在保持结构一致性的同时，优化三维高斯表示，从而在渲染质量上超越了以往的最先进方法。\n\nGaussianObject的核心贡献包括：提出了一种优化稀疏视角下三维高斯表示的方法，通过视觉外壳初始化和浮点消除训练来引入结构先验；基于扩散模型的高斯修复模型，用于去除由于物体信息遗漏或压缩造成的渲染伪影，进一步提升渲染质量。该框架在多个具有挑战性的真实世界数据集上进行了评估，包括MipNeRF360、OmniObject3D和OpenIllumination，结果显示GaussianObject在仅使用四张视角图像的情况下，能够显著优于现有技术，实现更高质量的三维物体重建。\n\n此外，研究还探讨了GaussianObject的局限性和未来工作方向，如减少对精确相机姿态的依赖、缓解极端视角下的伪影问题以及解决稳定扩散VAE中的颜色偏移问题。这些未来的研究方向有望进一步推动GaussianObject在日常生活中三维物体重建的应用，降低拍摄要求，拓宽应用前景。"}, "https://arxiv.org/abs/2402.10483": {"title_datetime_url": ["GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians", "2024.02.16", ""], "mechanism": "ShanghaiTech&nbsp;University:&nbsp;ShanghaiTech&nbsp;University||Huazhong&nbsp;University&nbsp;of&nbsp;Science&nbsp;and&nbsp;Technology:&nbsp;Huazhong&nbsp;University&nbsp;of&nbsp;Science&nbsp;and&nbsp;Technology||Deemos&nbsp;Technology:&nbsp;Deemos&nbsp;Technology||LumiAni&nbsp;Technology:&nbsp;LumiAni&nbsp;Technology", "summary": "GaussianHair是一种创新的头发建模和渲染技术，它通过将头发丝视为一系列连接的圆柱形3D高斯体来实现。这种方法不仅保留了头发的几何结构和外观，还支持高效的2D图像平面光栅化，便于实现可微分体积渲染。GaussianHair的核心是其独特的“GaussianHair散射模型”，它能够精确捕捉头发丝的纤细结构和局部漫反射颜色，从而在均匀光照下实现逼真的头发渲染。通过广泛的实验，GaussianHair在几何和外观真实性方面取得了突破性进展，超越了现有头发重建方法的局限性。\n\nGaussianHair的另一个显著特点是其对头发编辑、重照明和动态渲染的支持，这使得它能够无缝集成到传统的计算机图形（CG）工作流程中。为了进一步推动该领域的研究，研究者们还构建了一个包含真实人类头发的广泛数据集，每个样本都详细记录了头发丝的几何结构。这个数据集不仅包含了头发的几何信息，还包括了外观特征，为未来研究提供了宝贵的资源。\n\n尽管GaussianHair在头发建模方面取得了显著成就，但仍存在一些局限性。例如，其散射模型在物理原理上有所偏离，主要依赖于Unreal Engine的近似BSDF参数化和简化的多重散射方法。此外，某些物理属性（如粗糙度和反射指数）需要手动调整，这可能会影响结果的准确性。未来的研究可能会探索使用深度学习来优化这些参数，以及改进头发表示以提高渲染质量。尽管如此，GaussianHair为数字人类资产的高保真度建模和渲染开辟了新的可能性。"}}