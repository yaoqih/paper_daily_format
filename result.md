# Topic: Image Generation｜

## Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild

2024.01.24｜

&emsp;&emsp; 《Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild》是一篇关于图像恢复领域的研究论文，提出了一种名为SUPIR（Scaling-UP Image Restoration）的创新方法。SUPIR通过结合生成先验和模型扩展，显著提升了图像恢复的质量和智能化水平。该方法利用了多模态技术和先进的生成先验，通过收集包含描述性文本注释的2000万张高分辨率、高质量的图像进行模型训练，使得SUPIR能够根据文本提示指导图像恢复，从而拓宽了其应用范围和潜力。此外，研究者们还引入了负质量提示来进一步提高感知质量，并开发了一种基于恢复指导的采样方法来抑制生成过程中的保真度问题。实验结果表明，SUPIR在各种图像恢复任务中展现出卓越的恢复效果，尤其是在复杂和具有挑战性的现实世界场景中。

&emsp;&emsp;SUPIR的核心在于模型扩展，这不仅增强了其能力，还展示了图像恢复的新潜力。研究者们通过微调图像编码器，提高了其对图像退化变化的鲁棒性，并通过包含低质量、负样本的训练策略，显著改善了图像质量。这些策略，结合高效的工程实现，是实现SUPIR扩展的关键，推动了先进图像恢复技术的边界。SUPIR在图像恢复技术方面处于前沿地位，为未来的技术进步设定了新的基准。

&emsp;&emsp;论文还详细介绍了SUPIR的相关工作，包括图像恢复的目标、方法和挑战，以及模型扩展的重要性。SUPIR采用了StableDiffusion-XL（SDXL）作为强大的生成先验，包含26亿个参数，并设计了一个超过6亿参数的适配器来有效应用这一模型。此外，研究者们还探讨了如何通过文本提示灵活控制恢复过程，这极大地扩展了图像恢复的可能性。SUPIR模型在多种图像恢复任务中表现出色，特别是在处理严重退化的图像时，能够恢复出高质量的图像，同时保持对输入图像的忠实度。

## Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics

2024.01.24｜

&emsp;&emsp; 本研究提出了一种创新的音频辅助自动图像着色（AIAIC）方法，旨在通过利用与图像场景相对应的音频信息来提高着色准确性。传统的图像着色任务面临不确定性，需要对场景有精确的语义理解才能为灰度图像估计合理的颜色。尽管基于交互的方法取得了显著进展，但仅从视觉模态推断现实和准确的颜色仍然是一个挑战。为了减少语义理解的难度，研究者们尝试利用音频，它自然包含了关于同一场景的额外语义信息。

&emsp;&emsp;研究者们提出了一个三阶段的AIAIC网络。首先，使用彩色图像语义作为桥梁，预训练一个受彩色图像语义指导的着色网络。其次，利用音频和视频的自然共现性来学习音频和视觉场景之间的颜色语义关联。最后，将隐含的音频语义表示输入到预训练网络中，实现音频引导的着色。整个过程以自监督的方式进行训练，无需人工标注。此外，研究者们还建立了一个音频视觉着色数据集用于训练和测试。实验表明，音频引导可以有效提高自动着色的性能，尤其是在仅从视觉模态难以理解的场景中。

&emsp;&emsp;研究的贡献包括：首次采用跨模态音频信息辅助图像着色任务；在训练和测试阶段不涉及音频语义的标签，网络可以自监督地学习音频的潜在场景颜色语义，为视觉着色提供合理有效的指导；建立了一个用于音频引导图像着色任务的数据集，定量和定性实验结果表明所提出的方法优于现有方法。研究还探讨了音频场景语义的作用，并通过交换音频实验验证了模型学习音频场景颜色语义的能力。尽管如此，该方法在某些情况下可能导致场景颜色溢出或抑制其他对象的颜色，未来的工作将致力于通过构建更大分辨率的图像训练集来进一步提高性能。

## UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion

2024.01.24｜

&emsp;&emsp; UNIMO-G是一个创新的多模态条件扩散框架，旨在通过结合文本和视觉输入来生成图像。该框架由两个核心组件构成：一个多模态大型语言模型（MLLM）用于编码多模态提示，以及一个基于条件去噪扩散网络来生成图像。UNIMO-G通过两阶段训练策略有效训练：首先在大规模文本-图像对数据集上进行预训练，以发展条件图像生成能力；然后通过多模态提示进行指令调整，实现统一的图像生成能力。研究者们还设计了一个精心的数据处理流程，包括语言定位和图像分割，以构建多模态提示。

&emsp;&emsp;UNIMO-G在文本到图像生成和零样本主题驱动合成方面表现出色，尤其是在生成复杂多模态提示的高保真图像方面。它能够忠实地再现输入图像中的所有图像实体，渲染文本内容，并遵循多模态提示中的指示。在MS-COCO和DreamBench数据集上的评估表明，UNIMO-G在这些场景中的性能优于现有的视觉-语言到图像（VL2I）模型。此外，研究者们还引入了MultiBench，一个新的基准测试，专门用于评估多实体主题驱动生成，进一步证实了UNIMO-G在零样本多实体主题驱动生成中的有效性。

&emsp;&emsp;尽管UNIMO-G在图像生成的控制性和多模态指令遵循方面取得了显著进步，但它仍然存在一些局限性，如在复杂场景合成、视觉忠实度以及多实体图像生成任务中的挑战。此外，该技术可能被滥用于创建深度伪造图像，引发伦理问题。尽管如此，UNIMO-G展示了在图像生成过程中实现更细腻和可控过程的巨大潜力。

# Topic: 3D Reconstruction｜

## Style-Consistent 3D Indoor Scene Synthesis with Decoupled Objects

2024.01.24｜

&emsp;&emsp; 本研究提出了一种新颖的3D室内场景合成管道，旨在生成具有风格一致性的3D室内场景。该方法通过使用文本提示或单视图图像来控制场景中各个对象的生成和风格化，实现了对场景中多个对象的解耦和风格化。研究者们采用了网格（mesh）作为3D对象的表示形式，这允许对象无缝集成到下游应用中，如AR/VR设备。通过利用SyncDreamer模型，可以从单视图图像重建对象网格，从而显著扩展了可选择对象的范围。

&emsp;&emsp;研究的核心贡献包括：首先，提出了一个专门用于生成解耦网格对象的3D室内场景合成管道，这些对象可以通过文本提示或单视图图像进行风格化；其次，场景中的对象可以通过文本指令或风格图像进行风格化，确保多个对象之间保持一致的风格；最后，生成的完整室内场景在风格和空间布局上展现出视觉一致性，呈现出统一且美观的组合。

&emsp;&emsp;在实验部分，研究者们使用3D-FRONT数据集进行了评估，并与现有的3D场景合成和场景级网格风格化方法进行了比较。实验结果表明，该方法在风格一致性和视觉质量方面取得了显著提升。此外，通过用户研究，研究者们还展示了用户对场景对象进行操作（如重新排列、移除和克隆）的灵活性，以及对场景进行相机控制的能力。尽管如此，该方法仍存在一些局限性，如对整体场景风格监督的进一步探索，以及在对象排列优化算法方面的改进。未来的工作将集中在提高合成3D室内场景的审美质量上。

## Generative Human Motion Stylization in Latent Space

2024.01.24｜

&emsp;&emsp; 本研究提出了一种新颖的3D人类运动风格化框架，该框架利用预训练自编码器的潜在空间作为运动提取和注入的更表达性和鲁棒的表示。研究者们构建了一个生成模型，能够基于单个运动（潜在）代码产生多样化的风格化结果。在训练过程中，运动代码被分解为两个编码组件：一个确定性的内容代码和一个遵循先验分布的概率风格代码；然后生成器通过随机组合内容和风格代码来重构相应的运动代码。这种方法具有灵活性，允许从风格标记或未标记的运动中学习概率风格空间，提供了显著的风格化灵活性。

&emsp;&emsp;研究者们的方法在风格再现、内容保留和跨不同应用和设置的泛化方面表现出色。实验结果表明，尽管设计轻量级，但所提出的风格化模型在性能上超越了现有技术。此外，通过用户研究，研究者们展示了用户对场景对象进行操作（如重新排列、移除和克隆）的灵活性，以及对场景进行相机控制的能力。尽管如此，该方法仍存在一些局限性，如对整体场景风格监督的进一步探索，以及在对象排列优化算法方面的改进。未来的工作将集中在提高合成3D室内场景的审美质量上。

&emsp;&emsp;在实验部分，研究者们使用3D-FRONT数据集进行了评估，并与现有的3D场景合成和场景级网格风格化方法进行了比较。实验结果表明，该方法在风格一致性和视觉质量方面取得了显著提升。此外，通过用户研究，研究者们还展示了用户对场景对象进行操作（如重新排列、移除和克隆）的灵活性，以及对场景进行相机控制的能力。尽管如此，该方法仍存在一些局限性，如对整体场景风格监督的进一步探索，以及在对象排列优化算法方面的改进。未来的工作将集中在提高合成3D室内场景的审美质量上。

