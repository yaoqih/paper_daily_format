{"https://arxiv.org/abs/2401.13627": {"title_datetime_url": ["Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild", "2024.01.24", ""], "summary": " SUPIR（Scaling-UP Image Restoration）是一种创新的图像恢复方法，它通过结合生成性先验和模型扩展的威力，显著提升了智能和逼真图像恢复的能力。该方法由中国科学院深圳先进技术研究院、上海人工智能实验室、悉尼大学、香港理工大学和腾讯PCG ARC实验室共同开发。SUPIR的核心在于模型扩展，这一过程极大地增强了其能力，并展示了图像恢复的新潜力。研究团队收集了包含2000万张高分辨率、高质量的图像作为训练数据，并为每张图像提供了描述性文本注释，这为模型的训练提供了坚实的基础。此外，SUPIR还能够根据文本提示进行有针对性的图像恢复，从而拓宽了其应用范围和潜力。\n\nSUPIR在处理真实世界低质量图像时表现出卓越的恢复效果，如图1(a)所示。它还能够通过文本提示来指定恢复模糊物体（案例1）、定义物体的材质纹理（案例2）以及根据高级语义调整恢复（案例3）。为了提高图像质量，研究者们引入了负质量提示，并开发了一种基于恢复的采样方法来抑制生成过程中的保真度问题。实验结果表明，SUPIR在各种图像恢复任务中表现出色，尤其是在复杂和具有挑战性的现实世界场景中。\n\n尽管SUPIR在模型规模上取得了显著进步，但在追求规模扩大的同时，研究团队也面临了一系列复杂挑战。例如，为了使模型能够准确解释低质量图像的内容，他们对图像编码器进行了微调，以提高其对图像退化的鲁棒性。此外，为了解决模型规模过大难以训练的问题，他们设计了一个具有600多万参数的适配器，并采用了一种反直觉的策略，将质量较差的负样本纳入训练，以进一步提高视觉效果。这些策略，结合高效的工程实现，是实现SUPIR规模扩大、推动先进图像恢复技术边界的关键。", "mechanism": "SIAT,&nbsp;CAS,&nbsp;SAIL,&nbsp;USYD,&nbsp;HKPU,&nbsp;ARC&nbsp;Lab,&nbsp;Tencent&nbsp;PCG,&nbsp;CUHK"}, "https://arxiv.org/abs/2401.13270": {"title_datetime_url": ["Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics", "2024.01.24", ""], "summary": " 本文提出了一种新颖的音频增强自动图像着色（AIAIC）方法，旨在利用与图像相对应的音频信息来提高着色准确性。这种方法通过三个阶段实现：首先，使用彩色图像语义作为桥梁，预训练一个着色网络；其次，利用音频和视频的自然共现性学习音频和视觉场景之间的颜色语义关联；最后，将隐含的音频语义表示输入到预训练网络中，实现音频引导的着色。整个过程在无需人工标注的情况下进行自监督训练。此外，研究者还建立了一个音频视觉着色数据集用于训练和测试。实验表明，音频引导能有效提升自动着色的性能，尤其是在仅通过视觉模式难以理解的场景中。\n\n在相关工作部分，文章回顾了半自动着色和全自动着色方法，指出了现有方法在处理缺乏上下文线索的灰度图像时的局限性。为了解决这一问题，研究者提出了利用音频模态来补充和增强视觉语义。通过分析音频和视觉之间的自然场景语义联系，如雨滴声提示阴天，公鸡啼鸣让人联想到红冠的公鸡，研究者设计了一个双流网络，直接在端到端训练中融合音频和视觉特征。然而，由于音频和视觉之间的模态异质性，视觉骨干网络通常忽视音频语义的作用，这促使研究者采用一种基于场景语义的间接融合策略。\n\n在提出的AIAIC方法中，研究者首先通过预训练的语义引导着色网络学习颜色和场景语义之间的关系。然后，使用从彩色图像中提取的场景语义来指导音频语义的学习。最后，将学习到的音频语义表示输入到预训练的视觉着色网络中，以实现音频引导的着色。这种方法不仅提高了着色的语义准确性，而且在没有人工标注的情况下，通过自监督学习有效地提取了音频的潜在场景语义。实验结果表明，该方法在多个测试集上优于现有的自动着色方法，尤其是在处理具有强烈音频和视频信号对应关系的场景时。", "mechanism": "Hefei&nbsp;University&nbsp;of&nbsp;Technology,&nbsp;Peking&nbsp;University&nbsp;Shenzhen&nbsp;Graduate&nbsp;School"}, "https://arxiv.org/abs/2401.13388": {"title_datetime_url": ["UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion", "2024.01.25", "https://unimo-ptm.github.io/"], "summary": " UNIMO-G是一个创新的多模态条件扩散框架，旨在通过结合文本和视觉输入生成图像。该框架由两个核心组件构成：一个多模态大型语言模型（MLLM）用于编码多模态提示，以及一个条件去噪扩散网络用于基于编码的多模态输入生成图像。UNIMO-G通过两阶段训练策略进行有效训练：首先在大规模文本-图像对数据集上进行预训练，以发展条件图像生成能力，然后通过多模态提示进行指令调整，实现统一的图像生成能力。该框架在处理复杂多模态提示，尤其是涉及多个图像实体时，表现出生成高保真度图像的卓越能力。\n\nUNIMO-G在文本到图像生成和零样本主题驱动合成方面表现出色，特别是在生成复杂多模态提示的高保真度图像方面。它通过精心设计的数据处理流程，包括语言锚定和图像分割，构建多模态提示。在MS-COCO和DreamBench数据集上的评估显示，UNIMO-G在文本到图像和主题驱动生成场景中均优于现有模型。此外，为了评估其在多实体主题驱动生成方面的能力，研究者引入了MultiBench，一个新的基准测试，专门针对多实体场景。UNIMO-G在MultiBench上的表现进一步证实了其在零样本多实体主题驱动生成中的有效性。\n\n尽管UNIMO-G在图像生成领域取得了显著进展，但仍存在一些局限性，如在复杂场景合成、视觉忠实度以及多实体图像生成任务中的准确性不足。此外，该技术可能被滥用于创建深度伪造图像，引发伦理问题。尽管如此，UNIMO-G展示了在图像生成过程中实现更细致和可控过程的巨大潜力。", "mechanism": "Baidu&nbsp;Inc."}, "https://arxiv.org/abs/2401.13203": {"title_datetime_url": ["Style-Consistent 3D Indoor Scene Synthesis with Decoupled Objects", "2024.01.24", ""], "summary": " 本研究提出了一种新颖的3D室内场景合成管道，旨在通过文本提示或单视图图像生成具有风格一致性的多对象3D室内场景。该方法的核心在于将场景中的多样化对象进行分离，并利用专业设计的信息来战略性地放置这些对象。与传统的3D风格化方法相比，这项工作能够实现对单个对象的定制化风格化，同时保持整个场景的风格一致性。研究者们采用了网格（mesh）作为3D表示，这使得对象可以无缝集成到下游应用中，如AR/VR设备。此外，通过利用SyncDreamer的能力，可以从单视图图像重建单个网格，从而显著扩展了可选择对象的范围。\n\n研究者们的方法在3D场景合成领域取得了显著进展，特别是在多视图一致性和风格化方面。他们的方法不仅在视觉上令人印象深刻，而且展示了照片般的真实感、多视图一致性和多样性。这些场景是根据各种自然语言提示生成的，展示了模型的多样性和适应性。通过实验，研究者们展示了他们的方法在3D一致性风格化方面的改进，无论是定性还是定量评估。\n\n尽管取得了这些进展，研究者们也指出了他们的管道存在的局限性，并提出了未来的工作方向。他们认为，需要进一步探索整个场景的风格监督，并考虑将优化算法应用于对象排列，以提高合成3D室内场景的美学质量。此外，他们还计划在未来的研究中探索如何结合全球条件图像指导，以实现更高质量的场景合成。", "mechanism": "NTU,&nbsp;HKUST(GZ)"}, "https://arxiv.org/abs/2401.13505": {"title_datetime_url": ["Generative Human Motion Stylization in Latent Space", "2024.01.24", ""], "summary": " 本研究提出了一种新颖的生成性人类运动风格化方法，该方法在潜在空间中进行，旨在在保持输入运动内容不变的情况下，修改运动的风格。与现有工作直接在姿态空间操作不同，研究者利用预训练自编码器的潜在空间作为更表达性和鲁棒的运动提取和注入的表示。在此基础上，他们提出了一个生成模型，能够从单个运动（潜在）代码产生多样化的风格化结果。在训练过程中，运动代码被分解为两个编码组件：确定性的内容代码和遵循先验分布的概率风格代码；然后，生成器通过随机组合内容和风格代码来重建相应的运动代码。该方法具有灵活性，允许从风格标记或未标记的运动中学习概率风格空间，提供了显著的风格化灵活性。在推理阶段，用户可以选择使用参考运动或标签来风格化运动，甚至在没有明确风格输入的情况下，模型也能通过从无条件风格先验分布中采样来实现新颖的风格化。实验结果表明，尽管模型设计轻量级，但在风格再现、内容保持和跨各种应用和设置的泛化方面，所提出的模型性能优于现有技术。\n\n研究者在ICLR 2024上发表的这篇论文详细介绍了他们的生成性运动风格化框架。该框架通过将运动投影到潜在空间，并学习从输入代码中提取内容和风格，进一步支持在推理阶段的多种应用。模型架构包括风格编码器、内容编码器和生成器三个主要组件。风格编码器接受运动代码和风格标签作为输入，生成一个高斯分布来定义风格空间，而内容编码器则将运动代码转换为保持时间维度的内容代码。生成器通过自适应实例归一化（AdaIN）将内容和风格信息结合起来，生成有意义的运动代码。为了提高内容和风格的解耦，研究者提出了一种称为同风格对齐的技术，鼓励同一序列中不同运动子片段的风格空间接近。此外，全局运动通过预训练的全局运动回归器获得。\n\n在实验部分，研究者采用了三个数据集进行全面评估，包括广泛使用的运动风格数据集、较小的运动风格集合以及CMU Mocap（CMU）数据集。他们设计了一系列指标来全面评估方法，包括风格识别准确率、内容识别准确率、内容FID、风格FID和多样性。与现有技术相比，他们的方法在保持高风格准确率的同时，内容语义的损失最小。用户研究结果也表明，他们的方法在真实感和风格化质量方面得到了用户的青睐。此外，由于潜在风格化和轻量级网络设计，模型在推理阶段表现出较高的效率，显示出实时应用的潜力。", "mechanism": "University&nbsp;of&nbsp;Alberta,&nbsp;Noah's&nbsp;Ark&nbsp;Lab,&nbsp;Huawei&nbsp;Canada"}}