{"https://arxiv.org/abs/2402.04754": {"title_datetime_url": ["Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints", "2024.02.07", ""], "mechanism": "UB,&nbsp;Adobe", "summary": "这篇文章介绍了一种名为LACE（Layout Constraint Diffusion Model）的新型布局生成模型，它通过连续扩散模型来处理各种布局生成任务，包括指定属性的元素排列和粗糙布局设计的细化或完成。LACE模型基于连续扩散模型，与现有使用离散扩散模型的方法相比，连续状态空间设计允许在训练中整合可微分的美学约束函数。在条件生成方面，LACE通过掩蔽输入引入条件，实验结果表明，LACE能够产生高质量的布局，并在现有最先进基线模型上有所超越。\n\nLACE模型的核心贡献在于，它将各种可控布局生成任务作为连续空间中的条件生成过程，实现了约束优化以提高质量。模型提出了两种美学约束损失函数，分别促进全局对齐和最小化布局中的重叠。这些函数在训练和后处理阶段作为约束使用。此外，LACE通过时间依赖的权重来避免在噪声较大的时间步骤中收敛到局部最小值，并通过后处理阶段的全局对齐损失和成对重叠损失来进一步优化布局的美学质量。\n\n文章还详细介绍了LACE模型的方法论，包括连续扩散模型的基本原理、连续布局生成的实现、重建和美学约束的引入，以及模型架构和训练细节。通过在PubLayNet和Rico数据集上的实验，LACE展示了其在无条件和条件布局生成任务中的有效性，尤其是在全局对齐和最小化重叠方面的表现。尽管LACE在布局元素的表示灵活性、背景和内容感知以及元素数量限制方面存在局限性，但它为未来在更复杂和多样化的设计场景中的应用提供了新的方向。"}, "https://arxiv.org/abs/2402.04504": {"title_datetime_url": ["Text2Street: Controllable Text-to-image Generation for Street Views", "2024.02.07", ""], "mechanism": "Meituan", "summary": "文章《Text2Street: Controllable Text-to-image Generation for Street Views》提出了一个名为Text2Street的新型可控文本到图像生成框架，旨在解决基于文本生成街景图像的挑战。该框架针对街景图像生成中的三个主要难题：复杂的道路拓扑结构、多样的交通状况和多变的天气条件。为了应对这些挑战，Text2Street引入了三个关键组件：车道感知的道路拓扑生成器（LRTG）、基于位置的对象布局生成器（POLG）和多控制图像生成器（MCIG）。LRTG通过文本描述创建局部语义地图，精确生成道路结构和车道线；POLG则根据文本描述生成符合交通规则的交通对象布局；MCIG则整合道路拓扑、对象布局和天气描述，生成最终的街景图像。实验表明，Text2Street在生成街景图像方面取得了显著的进步，特别是在控制道路拓扑、交通对象数量和天气条件方面表现出色。\n\n在实验部分，作者使用公开的自动驾驶数据集nuScenes进行了验证，并通过与现有最先进方法的比较，展示了Text2Street在图像质量和文本到图像一致性方面的优越性。此外，作者还进行了消融实验，验证了各个组件的有效性，并展示了Text2Street在对象检测任务中的实用性，通过生成的图像补充训练数据，提高了YOLOv5模型的性能。最后，Text2Street还支持对生成的图像进行编辑，允许用户修改道路结构、车道线、对象布局和天气条件，进一步证明了其灵活性和实用性。\n\n总结来说，Text2Street框架通过创新的组件设计和多控制策略，有效地解决了街景图像生成中的复杂问题，实现了高质量的文本到图像转换，并在自动驾驶领域的数据增强和图像编辑方面展示了其潜力。这项工作不仅推动了文本到图像生成技术的发展，也为自动驾驶和机器人感知等领域的应用提供了新的可能性。"}, "https://arxiv.org/abs/2402.04625": {"title_datetime_url": ["Noise Map Guidance: Inversion with Spatial Context for Real Image Editing", "2024.02.07", ""], "mechanism": "Korea&nbsp;University,&nbsp;NAVER&nbsp;Cloud,&nbsp;POSTECH,&nbsp;Yonsei&nbsp;University", "summary": "1. 论文《Noise Map Guidance: Inversion with Spatial Context for Real Image Editing》提出了一种名为Noise Map Guidance (NMG)的新方法，旨在解决使用文本引导扩散模型进行真实图像编辑时遇到的挑战。这些挑战包括文本条件导致重建质量下降，进而影响编辑的准确性。NMG通过利用DDIM逆向过程中产生的噪声图（noise maps），这些噪声图本质上是图像的噪声表示，能够自然地捕捉空间上下文。NMG不需要优化步骤，而是通过条件化噪声图和文本嵌入来实现忠实的编辑，从而在不牺牲编辑质量的情况下，保持了编辑速度。\n\n2. NMG在多种编辑技术中的应用表现出色，包括与Prompt-to-Prompt、MasaCtrl和pix2pix-zero等方法的结合。实验结果表明，NMG在保持输入图像空间上下文的同时，能够成功进行视角转换、零样本图像到图像翻译等操作。此外，NMG在用户研究中也得到了较高的评价，用户更倾向于选择NMG编辑的图像，这表明NMG在人类感知的图像质量方面表现良好。\n\n3. 尽管NMG在真实图像编辑领域取得了显著进展，但仍存在一些局限性。例如，NMG依赖于文本进行图像编辑，这限制了其在进行精确空间变化时的能力。此外，NMG与某些不遵循逆向框架的方法（如SGC-Net）的兼容性有限，这在处理关系变化任务时尤为明显。未来的研究需要解决这些挑战，以实现更精确的空间控制和更广泛的应用。"}, "https://arxiv.org/abs/2402.04324": {"title_datetime_url": ["ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation", "2024.02.06", "https://tiger-ai-lab.github.io/ConsistI2V/"], "mechanism": "UW,&nbsp;VI,&nbsp;HA,&nbsp;UW,&nbsp;MAPRC", "summary": "CONSISTI2V是一种基于扩散模型的图像到视频（I2V）生成方法，旨在通过增强视觉一致性来解决现有方法在生成视频中保持主体、背景和风格完整性的挑战。该方法通过引入空间和时间上的注意力机制，以及从第一帧的低频带初始化噪声，来实现对视频生成过程中的视觉一致性增强。这些技术使得CONSISTI2V能够生成与第一帧高度一致的视频序列，并在自动和人工评估中表现出色，超越了现有方法。\n\n为了验证其有效性，研究者们提出了I2V-Bench，这是一个全面的评估基准，用于评价I2V生成模型。I2V-Bench包含了2951个高质量的YouTube视频，涵盖了16个不同类别，如风景、运动、动物和肖像等。评估框架包括视觉质量和视觉一致性两个维度，分别评估视频输出的感知质量和视频与文本提示的一致性。实验结果表明，CONSISTI2V在多个评估指标上均优于其他基线模型，特别是在动态度、背景一致性和对象一致性方面。\n\n此外，CONSISTI2V还展示了其在自动回归长视频生成和相机运动控制等应用中的潜力。通过使用FrameInit策略，模型能够在长视频生成中保持对象的一致性，并实现相机平移和缩放效果。尽管CONSISTI2V在某些情况下生成的视频运动幅度有限，且在训练数据集WebVid-10M中存在水印问题，但其在提高视频生成质量方面取得了显著进展，并为未来的研究提供了新的方向。"}, "https://arxiv.org/abs/2402.04717": {"title_datetime_url": ["InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior", "2024.02.07", "https://chenguolin.github.io/projects/InstructScene"], "mechanism": "PKU", "summary": "INSTRUCTSCENE是一个创新的3D室内场景生成框架，它通过整合语义图先验和布局解码器来提高场景合成的可控性和真实性。该框架能够理解自然语言指令，并在零样本（zero-shot）方式下，灵活地应用于多种下游任务。INSTRUCTSCENE首先基于用户指令设计一个整体的语义图，图中每个节点代表一个对象，边表示对象间的空间关系。然后，布局解码器利用这些结构化信息，生成与指令紧密相关的语义一致的场景。通过这种方法，INSTRUCTSCENE在生成控制性和真实性方面显著超越了现有技术。\n\nINSTRUCTSCENE的核心贡献包括提出了一种指令驱动的生成框架，该框架通过语义图先验联合模型外观和布局分布，以及创建了一个高质量的场景-指令对数据集，以促进指令驱动场景合成的基准测试。实验结果表明，INSTRUCTSCENE在多种房间类型的场景合成中，无论是在控制性还是真实性方面，都大幅度领先于现有方法。此外，INSTRUCTSCENE在零样本应用中也表现出色，能够在无需微调的情况下，处理风格化、重新排列、完成和无条件生成等任务。\n\n尽管INSTRUCTSCENE在3D室内场景合成方面取得了显著进展，但仍存在一些局限性。例如，它依赖于3D-FRONT数据集，该数据集可能包含一些错误的对象排列和分类。此外，当前的3D场景数据集规模相对较小，限制了模型的泛化能力。未来的工作可以探索如何扩展3D场景数据集的规模，或者利用大规模且良好标注的3D对象数据集来建立新的3D场景合成基准。INSTRUCTSCENE的语义图先验也为建模更复杂的户外场景提供了潜力，并且可以通过整合大型语言模型（LLMs）来进一步提升生成的控制性。"}, "https://arxiv.org/abs/2402.04796": {"title_datetime_url": ["Mesh-based Gaussian Splatting for Real-time Large-scale Deformation", "2024.02.07", ""], "mechanism": "UCAS,&nbsp;ICT,&nbsp;CityUHK,&nbsp;Cardiff", "summary": "本研究提出了一种基于网格的高斯溅射（Mesh-based Gaussian Splatting, MGS）方法，用于实现实时大规模变形。这种方法通过将3D高斯分布与显式网格结合，解决了传统高斯溅射（Gaussian Splatting, GS）在大规模变形时难以直接操作的问题。MGS通过在网格上定义3D高斯，并利用网格的拓扑结构来指导高斯分布的学习和变形过程，从而提高了渲染质量和变形效果。研究者们还引入了一种大规模高斯变形技术，通过调整与网格变形相关的3D高斯参数，实现了可编辑的GS。这种方法利用现有的网格变形数据集，实现了更真实的数据驱动高斯变形。\n\nMGS的核心思想是设计一种创新的基于网格的GS表示，该表示集成了高斯分布学习和操作。在3DGS学习过程中，高斯分裂被限制为两种选项：沿着网格面分裂和沿着法线分裂。这种分裂策略有助于形成更合理的操作并增强渲染效果，同时避免了在大规模变形时产生的不理性高斯（如错位高斯、长窄形状高斯）导致的伪影。基于所提出的网格基础GS，研究者们进一步引入了大规模高斯变形技术，该技术不仅利用顶点位置，还利用变形梯度来指导GS，同时保持了实时渲染和高质量外观。\n\n实验结果表明，MGS方法在保持高帧率（在单个NVIDIA RTX 4090 GPU上平均65 FPS）的同时，实现了高质量的重建和有效变形。与现有技术相比，MGS在变形效率和质量上表现出色，尤其是在大规模变形方面。此外，MGS还提供了一个交互式工具，允许用户通过拖动稀疏控制点实时操纵3DGS，实现了用户友好的约束。通过在公共数据集和自捕获场景上的广泛实验，MGS展示了其在新视角合成和大规模变形方面的优越性能。"}, "https://arxiv.org/abs/2402.05054": {"title_datetime_url": ["LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation", "2024.02.07", ""], "mechanism": "PKU,&nbsp;NTU,&nbsp;SAL", "summary": "本文介绍了一种名为Large Multi-View Gaussian Model (LGM)的新框架，旨在从文本提示或单视图图像生成高分辨率的3D模型。LGM的核心创新包括两个方面：首先，提出了多视图高斯特征作为高效的3D表示方法，这些特征可以通过可微渲染融合；其次，设计了一个不对称的U-Net作为高吞吐量的3D主干网络，用于处理多视图图像。这种方法在保持快速生成速度的同时，将训练分辨率提升至512，实现了高分辨率的3D内容生成。\n\nLGM框架通过两步生成3D模型：首先，利用现成的文本或图像到多视图扩散模型生成多视图图像；然后，使用基于U-Net的模型从这些稀疏视图图像中预测3D高斯。生成的高斯可以转换为多边形网格，以适应后续任务。为了增强训练的鲁棒性，LGM采用了数据增强技术，包括网格扭曲和轨道相机抖动。此外，LGM还提出了一种从3D高斯生成平滑纹理网格的有效方法。\n\n实验结果表明，LGM在图像到3D和文本到3D任务中都表现出色，生成的3D模型质量高、分辨率高且生成速度快。与现有方法相比，LGM在用户研究中获得了更高的评价，尤其是在图像一致性和整体质量方面。尽管LGM取得了显著进展，但仍存在一些局限性，如对输入视图质量的依赖、多视图扩散模型的不完美以及合成多视图图像分辨率的限制。未来的工作有望通过改进多视图扩散模型来缓解这些限制。"}}