# Topic: Image Generation｜T2I｜

## Improving Explicit Spatial Relationships in Text-to-Image Generation through an Automatically Derived Dataset

2024.03.01｜

<u>https://arxiv.org/abs/2403.00587</u>

本研究提出了一种改进文本到图像生成系统在表达显式空间关系（如“左边”或“下面”）方面的方法。研究者们发现，现有的文本到图像系统在处理这类空间关系时表现不佳，这可能是因为训练这些模型时使用的图像标题中很少包含显式空间关系。为了解决这个问题，研究者们提出了一种自动化方法，通过现有的图像生成包含14种显式空间关系的合成标题，创建了名为SR4G（Spatial Relation for Generation）的数据集。这个数据集包含了990万张图像-标题对用于训练，以及超过6万张标题用于评估。

研究者们通过在SR4G数据集上微调两种不同的稳定扩散模型（SDSR4G），展示了在VISOR度量上高达9点的改进。这种改进在未见过的分割（unseen split）中也得到了验证，表明SDSR4G能够泛化到未见过的物体上。与之前的研究相比，SDSR4G在参数更少的情况下提高了最先进的水平，并且避免了复杂的架构。

此外，研究者们还进行了详细的分析，显示改进在所有关系上都是一致的。他们还指出，尽管SDSR4G在某些空间关系（如“包围”和“内部”）上的表现仍有待提高，但总体而言，通过在SR4G数据集上的训练，模型在生成图像时能够更好地描绘出项目性和尺度关系，减少了对相反关系的偏见，并更好地泛化到在真实图像中更常见的空间三元组。未来的工作计划包括扩展关系集以包含深度信息，以及探索新的方法来收集和注释包含空间关系的自然标题。

## Diff-Plugin: Revitalizing Details for Diffusion-based Low-level Tasks

2024.03.01｜

<u>https://arxiv.org/abs/2403.00644</u>

本研究提出了Diff-Plugin框架，旨在提升预训练扩散模型在处理需要细节保留的低级视觉任务时的性能。Diff-Plugin通过一个轻量级的Task-Plugin模块和一个Plugin-Selector，使得单个预训练模型能够生成高保真度的结果，并适应多种低级任务。Task-Plugin模块包含双分支设计，提供任务特定的先验知识，引导扩散过程以保留图像内容。Plugin-Selector则根据文本指令自动选择不同的Task-Plugins，允许用户通过自然语言指示多个低级任务。

相比于以往的研究，Diff-Plugin在多个方面取得了显著改进。首先，它通过Task-Plugin模块有效地将任务特定的先验知识注入到扩散过程中，这有助于在不重新训练基础模型的情况下，为每个任务提供高保真度的视觉效果。其次，Plugin-Selector的引入，使得用户可以通过文本输入直观地驱动低级任务，增强了框架的实用性。此外，Diff-Plugin在处理复杂场景时表现出了优越的稳定性和可调度性，支持在不同数据集大小上进行稳健训练。

在实验中，Diff-Plugin在八种低级视觉任务上进行了广泛的测试，结果证明了其在真实世界场景中的优越性，特别是在内容一致性和细节保留方面。与现有的基于扩散和回归的方法相比，Diff-Plugin不仅在视觉上表现出色，而且在定量评估中也取得了竞争性的性能。此外，用户研究也表明，用户更倾向于选择Diff-Plugin进行图像编辑。尽管Diff-Plugin在局部编辑方面存在局限性，但研究者提出了可能的解决方案，例如整合大型语言模型来指示任务执行的区域。

# Topic: Image Generation｜Control, Editing, Personalization｜

## When ControlNet Meets Inexplicit Masks: A Case Study of ControlNet on its Contour-following Ability

2024.03.01｜

<u>https://arxiv.org/abs/2403.00467</u>

本研究提出了一种改进的ControlNet模型，名为Shape-aware ControlNet，旨在解决原始ControlNet在处理不精确（inexplicit）掩码时可能出现的图像质量下降问题。原始ControlNet在用户提供的精确轮廓掩码下表现出色，但当掩码含有噪声时，输出图像会出现不期望的伪影。为了提高ControlNet在处理不精确掩码时的可控性，研究者设计了一个包含退化评估器和形状先验调制块的高级Shape-aware ControlNet。退化评估器用于评估提供掩码的退化因子，然后这个因子被用于调制块中，以适应性地调节模型的轮廓跟随能力，帮助模型忽略不精确掩码中的噪声部分。

相比于之前的研究，Shape-aware ControlNet的主要改进在于引入了对掩码退化程度的量化控制。通过实验，研究者证明了该方法在鼓励ControlNet对不精确空间条件进行鲁棒解释方面的效果，而不是盲目跟随给定轮廓。此外，该方法还展示了在修改形状先验和可组合形状可控生成等应用场景中的应用潜力。

在实验部分，研究者通过与原始ControlNet的比较，展示了Shape-aware ControlNet在不同精度水平的掩码上的性能。结果表明，新方法在保持高图像保真度和空间控制的同时，能够更好地处理不精确的掩码。此外，研究者还提出了两个新的评估指标——布局一致性（LC）和语义检索（SR），以更全面地评估生成图像的质量。通过消融研究，研究者验证了退化评估器的准确性和形状先验调制块的鲁棒性。最后，研究者展示了该方法在处理TikZ草图、人类涂鸦以及可组合形状可控生成等创意应用中的有效性。

## LoMOE: Localized Multi-Object Editing via Multi-Diffusion

2024.03.01｜

<u>https://arxiv.org/abs/2403.00437</u>

LoMOE（Localized Multi-Object Editing via Multi-Diffusion）是一种新颖的图像编辑框架，它通过多扩散过程实现零样本（zero-shot）的局部多对象编辑。这项研究的主要贡献在于提出了一种能够在复杂场景中一次性对多个对象进行添加、替换或编辑的方法。与以往的基于文本提示的图像编辑方法相比，LoMOE利用前景掩模和简单的文本提示，对目标区域施加局部影响，从而实现高保真度的图像编辑。此外，LoMOE通过交叉注意力和背景保持损失，在潜在空间内确保被编辑对象的特性得以保留，同时实现高质量、无缝的背景重建，减少了与现有方法相比的伪影。

LoMOE的改进之处在于其能够处理复杂的局部对象细节，如心脏颜色、耳环、窗户视图、多云着色、画作中的动物类型以及树-动物类型等。这种方法通过结合全局提示来指导整体图像重建过程，确保前景和背景的一致性，同时最小化或使伪影不可察觉。LoMOE还引入了一个新的基准数据集LoMOE-Bench，用于评估现有框架的多对象编辑性能。实验结果表明，LoMOE在图像编辑质量和推理速度方面均优于现有最先进的方法。

LoMOE的提出，为艺术家和设计师提供了一个有效的工具，它不仅提高了图像编辑的质量和忠实度，而且通过一次性处理多个编辑任务，显著提高了推理速度。尽管LoMOE在处理对象删除或交换方面存在局限性，但其在图像编辑领域的潜力仍然巨大，为未来的研究提供了新的方向。

## RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization

2024.03.01｜

<u>https://corleone-huang.github.io/realcustom/</u>

本研究提出了RealCustom，这是一种新型的文本到图像定制方法，旨在实时为给定主题生成文本驱动的图像。与现有研究相比，RealCustom首次实现了主题相似性和文本可控性的解耦，通过精确限制主题影响范围仅在相关部分，解决了现有伪词范式中的双重最优悖论。这种方法通过逐步缩小真实文本词的一般含义到特定主题，并利用交叉注意力区分相关性，实现了高质量的相似性和可控性。

在训练阶段，RealCustom通过一种新颖的自适应评分模块学习视觉条件与原始文本条件之间的一般对齐能力，该模块能够根据文本和当前生成的特征调节影响量。在推理阶段，提出了一种自适应掩码引导策略，通过迭代更新给定主题的影响范围和影响量，逐步缩小真实文本词的生成。这种“训练-推理”解耦框架使得RealCustom在开放领域中展现出卓越的实时定制能力。

实验结果表明，RealCustom在开放领域中实现了前所未有的给定主题相似性和给定文本的可控性。与现有技术相比，RealCustom在CLIP图像分数（CLIP-I）和CLIP文本分数（CLIP-T）评估中均取得了显著提升，证明了其在实时开放领域定制能力上的优越性。此外，RealCustom的提出为实时开放领域定制提供了新的可能性，尤其是在处理罕见主题时，展现了出色的泛化能力。

# Topic: Image Generation｜Stylization｜

## Deformable One-shot Face Stylization via DINO Semantic Guidance

2024.03.01｜

<u>https://github.com/zichongc/DoesFS</u>

本研究提出了一种基于DINO语义指导的可变形一次性人脸风格化框架。该框架通过在单一真实风格图像对上进行训练，能够生成多样化、高质量的风格化人脸，同时保持输入身份的一致性。研究的核心是利用自监督视觉变换器DINO-ViT，建立跨真实和风格域的稳健且一致的面部结构表示。通过将空间变换器（STN）集成到StyleGAN生成器中，使生成器具有变形感知能力，并引入两个创新约束来优化生成器：方向变形损失和基于DINO特征自相似性的相对结构一致性约束，以确保多样化的生成结果。

相比于以往的研究，本方法在一次性人脸风格化方面取得了显著进步。传统的单图像风格参考方法主要关注颜色和纹理转移，而本研究强调了艺术风格中几何变形的重要性。通过使用真实风格图像对，而不是单一风格示例，研究者们能够构建跨域的可靠变形指导，从而减少了训练难度。此外，本研究还提出了基于DINO特征的两个新颖的跨域损失函数，以约束从真实面部到艺术风格的几何变形，并通过实验证明了其在外观变化和结构变形方面都能保持输入的真实身份。

在实验部分，研究者们通过定性和定量比较，展示了该框架在风格化效果上超越了现有的一次性人脸风格化方法。用户研究也表明，人们更倾向于选择本研究方法生成的风格化结果。此外，研究还探讨了损失项、STN模块和颜色对齐的效果，以及如何通过STN模块控制面部变形的程度。最后，研究者们感谢了支持该工作的多个科学基金和项目。

# Topic: Transformer｜Spatial Relations｜

## Can Transformers Capture Spatial Relations between Objects?

2024.03.01｜

<u>https://sites.google.com/view/spatial-relation</u>

本研究探讨了计算机视觉系统在识别物理基础的空间关系方面的能力。研究者们提出了精确的关系定义，并创建了一个基准数据集，以便一致地标注空间关系。尽管这项任务在视觉识别文献中相对简单，但现有方法在该基准上的表现不佳。研究者们提出了一种新的基于Transformer的方法，利用其长距离注意力机制来处理这项任务，并评估了关键的设计原则。他们发现了一个简单的“RelatiViT”架构，该架构在空间关系预测任务中超越了所有现有方法。这是首次有方法在野外设置中令人信服地超越了简单的基线。

研究者们通过实验比较了几种精心设计的基于Transformer的架构，并与现有的基于CNN的方法进行了对比，最终确定了“RelatiViT”架构。实验结果表明，RelatiViT在空间关系预测任务上显著优于现有方法，并且是第一个能够利用视觉信息显著提高性能的方法，而不仅仅依赖于对象的二维空间坐标。此外，即使是最先进的大规模视觉语言模型，如GPT-4V、Gemini、LLaVA和MiniGPT-4，在该基准上也未能取得令人满意的结果，这突显了空间关系预测对于视觉推理的重要性和挑战性。

这项工作不仅为空间关系预测任务提供了新的视角，而且为未来在这一领域的研究提供了重要的起点和基准。通过精确定义空间关系类别并重新标注SpatialSense数据集，研究者们解决了之前研究中存在的语言偏见和标注不一致性问题。RelatiViT架构的成功表明，端到端的Transformer模型能够有效地捕捉图像中的空间关系信息，这对于机器人操作和一般场景理解等应用至关重要。

# Topic: Video｜Video LLMs, Listening Head｜

## TempCompass: Do Video LLMs Really Understand Videos?

2024.03.01｜

<u>https://github.com/llyx97/TempCompass</u>

本研究提出了一个新的基准测试TempCompass，旨在全面评估视频大型语言模型（Video LLMs）对视频时间动态的理解能力。与现有基准测试相比，TempCompass在两个主要方面进行了改进：首先，它引入了多样化的时间维度，如动作、速度、方向、属性变化和事件顺序，以及更细粒度的子维度，从而能够更全面地诊断模型在特定时间感知方面的表现。其次，TempCompass涉及四种不同的任务格式，包括多选问答、是非问答、标题匹配和标题生成，这有助于研究者了解模型在不同任务格式下的时间感知性能如何变化。

为了收集高质量的测试数据，研究者设计了两种新颖的策略。在视频收集方面，他们构建了具有相同静态内容但在特定时间维度上有所不同的冲突视频对/三元组，以防止Video LLMs利用单帧偏差或语言先验。在任务指令收集方面，提出了一种范式，首先由人类为视频注释元信息，然后由LLM生成指令。此外，研究者还设计了一种基于LLM的方法，用于自动准确地评估Video LLMs的响应。

基于TempCompass基准测试，研究者全面评估了8种最先进的Video LLMs和3种Image LLMs，并揭示了一个重要事实：这些模型在时间感知能力方面表现出明显的不足。研究结果强调了在评估过程中纳入多样化任务格式的必要性，并指出了现有Video LLMs在时间感知方面亟待提升的领域。

## CustomListener: Text-guided Responsive Interaction for User-friendly Listening Head Generation

2024.03.01｜

<u>https://arxiv.org/abs/2403.00274</u>

本研究提出了一个名为CustomListener的用户友好框架，旨在通过文本引导的方式生成能够响应说话者的非言语听众头部动作。与以往的研究相比，CustomListener不仅能够通过简单的情感标签操纵动作，还能让用户自由定制听众代理的人类属性，如身份、个性等，从而提高真实感。研究者设计了静态到动态肖像模块（SDP）和过去引导生成模块（PGG），以实现说话者与听众之间的协调和动作连贯性。通过在ViCo和RealTalk数据集上进行的广泛实验，验证了模型的有效性。

CustomListener的改进之处在于，它通过大型语言模型结合用户定制的属性和说话内容，生成听众的基本肖像文本先验。这使得听众的动作不仅能够完成文本指定的动作，还能根据说话者的语义、语调和动作幅度进行调整，实现与说话者的同步。此外，PGG模块通过维护用户定制的听众行为习惯，确保了在长视频中不同文本先验条件下的动作连贯性。

在与现有方法的比较中，CustomListener在多个指标上表现出色，包括自然度、多样性、与说话者动作的同步性以及长期生成中的动作连贯性。这些改进使得CustomListener在生成可控且互动的听众动作方面达到了最先进的性能。未来的工作可能会将该框架推广到听众身体的生成，以创造出更完整和真实的听众代理。

# Topic: 3D Generation｜Shape｜

## HyperSDFusion: Bridging Hierarchical Structures in Language and Geometry for Enhanced 3D Text2Shape Generation

2024.03.01｜

<u>https://arxiv.org/abs/2403.00372</u>

HyperSDFusion是一种新型的3D文本到形状生成方法，它通过在双分支扩散模型中引入超空间学习，来桥接语言和几何的层级结构。该方法首先通过超空间文本-图像编码器学习文本的序列和多模态层级特征，然后设计了一个超空间文本图卷积模块来学习文本的层级特征。为了在3D特征空间中嵌入文本特征，引入了双分支结构。最后，为了赋予生成的3D形状以层级结构，提出了一种超空间层级损失函数。与现有的Text2Shape方法相比，HyperSDFusion在实验中取得了最先进的结果，特别是在捕捉文本和形状的层级特性方面表现出色。

HyperSDFusion的关键创新在于利用超空间来学习文本和形状的固有层级结构。通过引入超空间文本-图像编码器（HTIE）和超空间文本图卷积模块（HTGC），该方法能够更好地捕捉文本的序列和层级特征。此外，通过双分支扩散模型，HyperSDFusion能够独立地利用两个条件（HTIE和HTGC）进行去噪，这有助于更有效地利用文本特征。在实验中，与SDFusion相比，HyperSDFusion在IoU、CD和F-score等评估指标上都有显著提升，显示出在生成质量和层级结构保持方面的改进。

在超空间学习方面，HyperSDFusion通过超空间层级损失函数来监督3D特征空间的层级结构。这种损失函数有助于在去噪过程中保持特征的树状层级结构。实验结果表明，与SDFusion相比，HyperSDFusion在保持3D形状层级结构方面表现更好。通过可视化分析，可以看出HyperSDFusion生成的3D形状在从一般文本到详细文本的层次结构上与文本描述更为一致，而SDFusion则在这方面表现不佳。这些改进使得HyperSDFusion在文本到形状的生成任务中取得了显著的进步。

# Topic: 3D Graphs｜Surface Normal Estimation｜

## Rethinking Inductive Biases for Surface Normal Estimation

2024.03.01｜

<u>https://github.com/baegwangbin/DSINE</u>

本研究提出了一种新的方法，用于改进表面法线估计的归纳偏差。与以往使用通用密集预测模型的方法不同，作者提出了两个关键的改进：首先，利用每个像素的射线方向作为输入，以实现对相机内参的感知推断，从而提高模型的泛化能力；其次，通过学习相邻表面法线之间的相对旋转关系，编码像素间的关系，使得模型能够生成既平滑又在表面交界处清晰的预测结果。这种方法在面对任意分辨率和宽高比的野外图像时，展现出了强大的泛化能力。

相比于现有的基于ViT（Vision Transformer）的最先进的模型，该方法在数据集规模小得多的情况下，仍然能够展现出更强的泛化能力和更高的预测细节水平。这得益于其全卷积架构，使得模型能够处理任意分辨率和宽高比的图像，无需进行图像重置或位置编码的插值。此外，该模型的训练过程更为简单和高效，不需要复杂的3D数据增强，也不需要额外的监督信号。

研究还进行了消融实验，证明了所提出的归纳偏差的有效性。实验结果表明，加入每个像素的射线方向作为输入和通过迭代旋转估计更新初始预测，都能显著提高预测的准确性。尤其是在处理具有非标准相机内参的数据集（如Sintel和Virtual KITTI）时，射线方向编码的效果更为明显。尽管旋转估计带来的提升不是特别显著，但定性比较显示，通过旋转估计的细化，可以改善表面法线的一致性，并在物体边界附近产生更清晰的预测结果。

# Topic: AI for Science｜

## Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models

2024.03.01｜

<u>https://mm-arxiv.github.io</u>

本研究介绍了Multimodal ArXiv数据集，旨在提升大型视觉-语言模型（LVLMs）在科学领域的理解能力。该数据集由ArXivCap和ArXivQA两部分组成，ArXivCap包含来自572K篇科学论文的6.4M张图像和3.9M条标题，而ArXivQA是基于科学图像生成的10K个多项选择题对。通过在ArXivQA上的训练，LVLMs在多模态数学推理基准测试中实现了10.4%的准确率提升。此外，研究还设计了四个视觉到文本的任务，以评估LVLMs对科学图像的理解，发现领域特定训练能显著提高性能。

相比于以往的研究，Multimodal ArXiv数据集的规模更大，覆盖了更广泛的科学领域，提供了更真实的学术图像和标题。这使得LVLMs能够在更接近实际应用的环境中进行训练和评估。此外，通过GPT-4V生成的ArXivQA数据集，不仅增强了模型对科学图像的理解，还提高了其数学推理能力，这在以往的研究中较少涉及。

研究还通过错误分析揭示了当前LVLMs在理解视觉上下文、识别错误以及生成过于简化的标题方面的挑战。这些发现为未来LVLMs的发展提供了宝贵的见解，指出了需要进一步改进的方向，如整合更多上下文信息、提高模型的基本感知能力以及利用外部信息来提升模型性能。

